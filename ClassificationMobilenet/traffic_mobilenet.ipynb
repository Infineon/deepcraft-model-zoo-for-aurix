{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright (C) Infineon Technologies AG 2025\n",
    " \n",
    "Copyright (c) 2025, Infineon Technologies AG, or an affiliate of Infineon Technologies AG. All rights reserved.\n",
    "This software, associated documentation and materials (\"Software\") is owned by Infineon Technologies AG or one of its affiliates (\"Infineon\") and is protected by and subject to worldwide patent protection, worldwide copyright laws, and international treaty provisions. Therefore, you may use this Software only as provided in the license agreement accompanying the software package from which you obtained this Software. If no license agreement applies, then any use, reproduction, modification, translation, or compilation of this Software is prohibited without the express written permission of Infineon.\n",
    "\n",
    "Disclaimer: UNLESS OTHERWISE EXPRESSLY AGREED WITH INFINEON, THIS SOFTWARE IS PROVIDED AS-IS, WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ALL WARRANTIES OF NON-INFRINGEMENT OF THIRD-PARTY RIGHTS AND IMPLIED WARRANTIES SUCH AS WARRANTIES OF FITNESS FOR A SPECIFIC USE/PURPOSE OR MERCHANTABILITY. Infineon reserves the right to make changes to the Software without notice. You are responsible for properly designing, programming, and testing the functionality and safety of your intended application of the Software, as well as complying with any legal requirements related to its use. Infineon does not guarantee that the Software will be free from intrusion, data theft or loss, or other breaches (\"Security Breaches\"), and Infineon shall have no liability arising out of any Security Breaches. Unless otherwise explicitly approved by Infineon, the Software may not be used in any application where a failure of the Product or any consequences of the use thereof can reasonably be expected to result in personal injury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Structure\n",
    "\n",
    "1. **MobileNet for Traffic Object Classification**: Generation of the Model \n",
    "2. **Compiling the Model for AURIX&trade; Microcontrollers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNet for Traffic Object Classification\n",
    "\n",
    "In this notebook we will adapt the MobileNet so that typical traffic objects such as bikes and cars will be classified. Firstly, we will import the MobileNetV3-small with pretrained weights. Next, we will adapt the model so that only traffic-related classes will be classified. This will not only reduce the required storage space for the model but also maximize its accuracy. Finally, we will generate the code that can later be deployed on the AURIX&trade; TC4x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "\n",
    "Importing all required modules and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from torchsummary import summary\n",
    "import modelling_helper as mh\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from CentralScripts.python_flask_client import CallTools\n",
    "import CentralScripts.helper_functions as cs\n",
    "\n",
    "device = cs.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the MobileNetV3-small [1]. It's pretrained for 1000 classes using the ImageNet1K data set [2]. Since for traffic object classification not all the classes are relevant. The model will be adapted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    "\n",
    "weights = MobileNet_V3_Small_Weights.DEFAULT\n",
    "model_original = mobilenet_v3_small(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the traffic_classes.json file which contains all the indices of the MobileNetV3 which are relevant for traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = mh.get_classes_json(\"traffic_classes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching some data for testing from kagglehub (Sautkin data set [3]). Plotting some random input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = mh.get_data()\n",
    "_, input_size = mh.get_random_input(folder_path, classes, is_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting the model so that it only represents the traffic classes. For this, all the other classes will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"traffic_mobilenet\"\n",
    "model = mh.get_model_indices(model_original, classes)\n",
    "summary(model, input_size, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing random input which can later be tested on the hardware. Running inference on input data to get some matching output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_target, input_size = mh.get_random_input(folder_path, classes)\n",
    "output_target, predicted_class = mh.get_predicted_class(input_target, model, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model, input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.save_all(model_name, input_target, output_target, model, origin=\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model performance by presenting the prediction of a few random images from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.calculate_predictions_and_plot(model, folder_path, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model for validation by calculating the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.calculate_confusion_matrix(folder_path, model, device, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model for AURIX&trade; Microcontrollers\n",
    "\n",
    "For the compilation, we have to prepare a Docker container which includes all the required tools for compilation. In this section, we will set up the container and make sure that it's available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.ensure_docker_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model for two different targets from the AURIX&trade; microcontroller family: TriCore TC3x and TC4x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder, onnx_model_file = cs.get_output_paths(model_name)\n",
    "\n",
    "for target in [\"TC3\", \"TC4\"]:\n",
    "    tool = CallTools(\n",
    "        folder=model_folder, url=\"http://localhost:8080/convert\", target=target\n",
    "    )\n",
    "    tool.convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the run time of the layers using AURIX&trade; TC3x and TC4x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.plot_execution_timing(model_name, is_small_font=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]: Howard, A. et al. (2019). \"Searching for MobileNetV3.\", ICCV.<br>\n",
    "[2]: Russakovsky, O. et al. (2015). \"ImageNet Large Scale Visual Recognition Challenge\", IJCV<br>\n",
    "[3]: Sautkin data set, [Link to Kaggle](https://www.kaggle.com/datasets/sautkin/imagenet1kvalid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
