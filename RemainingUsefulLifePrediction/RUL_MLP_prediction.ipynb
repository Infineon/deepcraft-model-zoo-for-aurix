{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e3dc78",
   "metadata": {},
   "source": [
    "### Copyright (C) Infineon Technologies AG 2025\n",
    " \n",
    "Copyright (c) 2025, Infineon Technologies AG, or an affiliate of Infineon Technologies AG. All rights reserved.\n",
    "This software, associated documentation and materials (\"Software\") is owned by Infineon Technologies AG or one of its affiliates (\"Infineon\") and is protected by and subject to worldwide patent protection, worldwide copyright laws, and international treaty provisions. Therefore, you may use this Software only as provided in the license agreement accompanying the software package from which you obtained this Software. If no license agreement applies, then any use, reproduction, modification, translation, or compilation of this Software is prohibited without the express written permission of Infineon.\n",
    "\n",
    "Disclaimer: UNLESS OTHERWISE EXPRESSLY AGREED WITH INFINEON, THIS SOFTWARE IS PROVIDED AS-IS, WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ALL WARRANTIES OF NON-INFRINGEMENT OF THIRD-PARTY RIGHTS AND IMPLIED WARRANTIES SUCH AS WARRANTIES OF FITNESS FOR A SPECIFIC USE/PURPOSE OR MERCHANTABILITY. Infineon reserves the right to make changes to the Software without notice. You are responsible for properly designing, programming, and testing the functionality and safety of your intended application of the Software, as well as complying with any legal requirements related to its use. Infineon does not guarantee that the Software will be free from intrusion, data theft or loss, or other breaches (\"Security Breaches\"), and Infineon shall have no liability arising out of any Security Breaches. Unless otherwise explicitly approved by Infineon, the Software may not be used in any application where a failure of the Product or any consequences of the use thereof can reasonably be expected to result in personal injury."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93153582",
   "metadata": {},
   "source": [
    "# Remaining Useful Life (RUL) Prediction with MLP\n",
    "\n",
    "This notebook demonstrates how to build and train a Multi-Layer Perceptron (MLP) for remaining useful life prediction using the NASA Turbofan Engine dataset and the helper functions from `modelling_helper.py`.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Remaining Useful Life (RUL) prediction is a key technology for predictive maintenance systems across industrial sectors. In automotive applications, accurate RUL estimation provides:\n",
    "\n",
    "- **Proactive maintenance scheduling** - Reducing unplanned downtime through failure prediction\n",
    "- **Cost optimization** - Extending component operational life while avoiding premature replacement\n",
    "- **Safety improvement** - Early identification of potential component failures\n",
    "- **Fleet optimization** - Data-driven maintenance scheduling across vehicle fleets\n",
    "- **Warranty analysis** - Understanding component degradation patterns for design optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6af0c-2f20-4f04-a3b9-dc660841c452",
   "metadata": {},
   "source": [
    "### Notebook Structure\n",
    "\n",
    "1. **Import Libraries and Helper Functions**\n",
    "2. **Load and Explore the NASA Turbofan Engine Dataset**\n",
    "3. **Data Visualization**\n",
    "4. **Data Preprocessing for MLP**\n",
    "5. **Create and Build MLP Model**\n",
    "6. **Train the Model**\n",
    "7. **Model Export and Conversion**\n",
    "8. **Model Evaluation**\n",
    "9. **Compiling the Model for AURIX&trade;**\n",
    "10. **Evaluate Execution Timing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42154fa6",
   "metadata": {},
   "source": [
    "## Import Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path so we can import from central_scripts\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from CentralScripts.helper_functions import *\n",
    "from modelling_helper import *\n",
    "\n",
    "origin = \"torch\"\n",
    "\n",
    "train_data_path = \"data/train_FD001.txt\"\n",
    "test_data_path = \"data/test_FD001.txt\"\n",
    "test_RUL_path = \"data/RUL_FD001.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecca5441",
   "metadata": {},
   "source": [
    "## Load and Investigate the NASA Turbofan Engine Dataset\n",
    "\n",
    "This project uses the **NASA Turbofan Engine Degradation Simulation Dataset** (also known as the NASA C-MAPSS dataset). The dataset contains:\n",
    "\n",
    "- **Training data**: Run-to-failure time series data from multiple aircraft engines\n",
    "- **Test data**: Partial time series data for engines with unknown remaining useful life\n",
    "- **Ground truth**: Actual RUL values for the test engines\n",
    "\n",
    "The data loading process includes:\n",
    "- **Automatic download**: If data files are not found locally, they are automatically downloaded from the official NASA repository\n",
    "- **Data normalization**: For better model training the train data is normalized\n",
    "- **Feature filtering**: Remove features that do not carry meaningful information by checking the standard deviation of the data. Features with a standard deviation lower than 0.02 are excluded\n",
    "\n",
    "**Data Source**: The dataset is automatically downloaded from the [NASA Prognostics Data Repository](https://www.nasa.gov/content/prognostics-center-of-excellence-data-set-repository) when needed.\n",
    "\n",
    " A. Saxena and K. Goebel (2008). “Turbofan Engine Degradation Simulation Data Set”, NASA Prognostics Data Repository, NASA Ames Research Center, Moffett Field, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39f89a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataframe(train_data_path)\n",
    "train_data, scaler = normalize_data(train_data)\n",
    "\n",
    "removed_cols = clean_data(train_data)\n",
    "train_data.drop(removed_cols, axis=1, inplace=True)\n",
    "train_data = add_rul(train_data)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Removed columns: {removed_cols}\")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f88f5",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "- Time series patterns\n",
    "- RUL distribution\n",
    "- Sensor correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b582bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_cols = [\n",
    "    col for col in train_data.columns if \"sensor\" in col or \"op_setting\" in col\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(sensor_cols), 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "for i, sensor in enumerate(sensor_cols):\n",
    "    for unit in train_data[\"unit_number\"].unique():\n",
    "        unit_data = train_data[train_data[\"unit_number\"] == unit]\n",
    "        axes[i].plot(unit_data[\"time_in_cycles\"], unit_data[sensor])\n",
    "    axes[i].set_ylabel(sensor)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time in Cycles\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c1099",
   "metadata": {},
   "source": [
    "## Data Preprocessing for MLP\n",
    "\n",
    "- Turn into sequences of a defined length. The training data will have the shape `n_samples * sequence_length * n_features`\n",
    "- Split into training and validation data\n",
    "- For MLP, sequences will be flattened into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b581312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sequence_length = 50\n",
    "# Prepare sequences\n",
    "X, y = prepare_train_sequences(train_data, sequence_length)\n",
    "\n",
    "# Split the data into 90% training and 10% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"Number of features per sequence: {X_train.shape[1] * X_train.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70362bed",
   "metadata": {},
   "source": [
    "## Create and Build MLP Model\n",
    "\n",
    "- Tweak the `layer_units` list to test the effect of the model architecture on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921605d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# MLP architecture parameters\n",
    "layer_units = [128, 64, 32]  # Hidden layer units\n",
    "input_size = X_train.shape[1] * X_train.shape[2]  # Flattened sequence\n",
    "output_size = 1  # Single RUL value\n",
    "dropout_rate = 0.05\n",
    "\n",
    "model = MLPmodel(\n",
    "    layer_units=layer_units,\n",
    "    input_size=input_size,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=dropout_rate,\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model type: {model.model_type}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Layer units: {layer_units}\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fcbb7f",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "- Flatten the input data for MLP processing\n",
    "- The training process uses early stopping to avoid overfitting.\n",
    "- To ensure proper convergence learning rate reduction is used when training stalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten input data for MLP\n",
    "X_train_final = X_train.reshape(X_train.shape[0], -1)  # Flatten for MLP\n",
    "X_val_final = X_val.reshape(X_val.shape[0], -1)  # Flatten for MLP\n",
    "\n",
    "print(\"Flattened input for MLP:\")\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"X_val_final shape: {X_val_final.shape}\")\n",
    "\n",
    "train_loss, val_loss, lr = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    initial_lr=0.01,\n",
    "    num_epochs=10,  # Reduced for faster training in demo\n",
    ")\n",
    "\n",
    "plot_training_progress(train_loss, val_loss, lr)\n",
    "\n",
    "# Load the best model - check if file exists first\n",
    "checkpoint_path = \"model_checkpoints/best_model.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(\"Best model loaded.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Using current model state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5dc4f7",
   "metadata": {},
   "source": [
    "# Export Model to ONNX\n",
    "\n",
    "- Generate a model name based on its architecture and loss\n",
    "- Save the model together with a test in- & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5be73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model name\n",
    "model_name = f\"{model.model_type}_layers-{len(layer_units)}_loss-{min(val_loss):.1f}\"\n",
    "model_name = model_name.replace(\".\", \"-\")\n",
    "\n",
    "# Prepare input and output for model export\n",
    "input_target = X_train[0].astype(np.float32)  # Single sequence\n",
    "input_target = input_target.flatten()  # Flatten the input for MLP\n",
    "output_target = get_predictions(origin, model, input_target)\n",
    "\n",
    "save_all(model_name, input_target, output_target, model, origin)\n",
    "\n",
    "print(f\"Model saved as: {model_name}\")\n",
    "print(f\"Input shape for export: {input_target.shape}\")\n",
    "print(f\"Output shape: {output_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1f181",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Load Test Data\n",
    "\n",
    "Data that was not used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bda7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_dataframe(test_data_path)\n",
    "test_RUL = pd.read_csv(test_RUL_path, names=[\"RUL\"], header=None)\n",
    "test_data, scaler = normalize_data(test_data, scaler)\n",
    "\n",
    "test_data.drop(removed_cols, axis=1, inplace=True)\n",
    "\n",
    "X_test, y_test = prepare_test_sequences(test_data, test_RUL, sequence_length)\n",
    "\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa015f",
   "metadata": {},
   "source": [
    "### Make Model Predictions\n",
    "\n",
    "- Fix the model batch size to 1\n",
    "- Simplify the model\n",
    "- Make predictions with the original and simplified models\n",
    "- Compare the predictions and check if they are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dee75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder, onnx_model_file = get_output_paths(model_name)\n",
    "y_pred_test = pd.DataFrame(\n",
    "    {\n",
    "        \"y_pred\": predict_with_onnx(\n",
    "            onnx_model_file, X_test.reshape(X_test.shape[0], -1)\n",
    "        ).reshape(\n",
    "            -1,\n",
    "        ),\n",
    "        \"y\": y_test,\n",
    "        \"Train_test\": \"Test\",\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred_train = pd.DataFrame(\n",
    "    {\n",
    "        \"y_pred\": predict_with_onnx(\n",
    "            onnx_model_file, X_train.reshape(X_train.shape[0], -1)\n",
    "        ).reshape(\n",
    "            -1,\n",
    "        ),\n",
    "        \"y\": y_train,\n",
    "        \"Train_test\": \"Train\",\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred = pd.concat([y_pred_train, y_pred_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba4c55",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "- Mean squared error (MSE) as loss\n",
    "- R2 as statistical metric for the predictive quality of the model, R2 = 1 indicating a perfect model\n",
    "- Compare the performance on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382cb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_df = calculate_kpis(y_pred)\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(kpi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574054e",
   "metadata": {},
   "source": [
    "### Plot true vs. predicted RUL\n",
    "\n",
    "- Assess model performance by directly comparing true and predicted values\n",
    "- Better correlation indicates a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5b1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_vs_predicted(y_pred, kpi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f2e11",
   "metadata": {},
   "source": [
    "## Compiling the Model for AURIX&trade;\n",
    "\n",
    "We now generate a C file from the ONNX and convert this into an elf file next. The elf file can be deployed on the CPUs of an AURIX&trade; TC3.\n",
    "\n",
    "- Specify the `model_folder` and the `target` hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_docker_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e490c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CentralScripts.python_flask_client import CallTools\n",
    "\n",
    "model_folder, onnx_model_file = get_output_paths(model_name)\n",
    "\n",
    "for target in [\"TC3\", \"TC4\"]:\n",
    "    tool = CallTools(\n",
    "        folder=model_folder, url=\"http://localhost:8080/convert\", target=target\n",
    "    )\n",
    "    tool.convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb2de3b",
   "metadata": {},
   "source": [
    "## Evaluate Execution Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da38086",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_execution_timing(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cd366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
