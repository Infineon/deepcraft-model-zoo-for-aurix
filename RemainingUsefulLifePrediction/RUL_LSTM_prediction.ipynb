{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68f33a7",
   "metadata": {},
   "source": [
    "### Copyright (C) Infineon Technologies AG 2025\n",
    " \n",
    "Copyright (c) 2025, Infineon Technologies AG, or an affiliate of Infineon Technologies AG. All rights reserved.\n",
    "This software, associated documentation and materials (\"Software\") is owned by Infineon Technologies AG or one of its affiliates (\"Infineon\") and is protected by and subject to worldwide patent protection, worldwide copyright laws, and international treaty provisions. Therefore, you may use this Software only as provided in the license agreement accompanying the software package from which you obtained this Software. If no license agreement applies, then any use, reproduction, modification, translation, or compilation of this Software is prohibited without the express written permission of Infineon.\n",
    "\n",
    "Disclaimer: UNLESS OTHERWISE EXPRESSLY AGREED WITH INFINEON, THIS SOFTWARE IS PROVIDED AS-IS, WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ALL WARRANTIES OF NON-INFRINGEMENT OF THIRD-PARTY RIGHTS AND IMPLIED WARRANTIES SUCH AS WARRANTIES OF FITNESS FOR A SPECIFIC USE/PURPOSE OR MERCHANTABILITY. Infineon reserves the right to make changes to the Software without notice. You are responsible for properly designing, programming, and testing the functionality and safety of your intended application of the Software, as well as complying with any legal requirements related to its use. Infineon does not guarantee that the Software will be free from intrusion, data theft or loss, or other breaches (\"Security Breaches\"), and Infineon shall have no liability arising out of any Security Breaches. Unless otherwise explicitly approved by Infineon, the Software may not be used in any application where a failure of the Product or any consequences of the use thereof can reasonably be expected to result in personal injury."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a824a80",
   "metadata": {},
   "source": [
    "# Remaining Useful Life (RUL) Prediction with LSTM\n",
    "\n",
    "This notebook demonstrates how to build and train a Long Short-Term Memory (LSTM) neural network for remaining useful life prediction using the NASA Turbofan Engine dataset and the helper functions from `modelling_helper.py`.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Remaining Useful Life (RUL) prediction is a key technology for predictive maintenance systems across industrial sectors. In automotive applications, accurate RUL estimation provides:\n",
    "\n",
    "- **Proactive maintenance scheduling** - Reducing unplanned downtime through failure prediction\n",
    "- **Cost optimization** - Extending component operational life while avoiding premature replacement\n",
    "- **Safety improvement** - Early identification of potential component failures\n",
    "- **Fleet optimization** - Data-driven maintenance scheduling across vehicle fleets\n",
    "- **Warranty analysis** - Understanding component degradation patterns for design optimization\n",
    "\n",
    "LSTM networks are particularly well-suited for RUL prediction as they can capture long-term dependencies in time series data and model complex degradation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c7f5f-278a-4625-82a5-423b1cf89b52",
   "metadata": {},
   "source": [
    "### Notebook Structure\n",
    "\n",
    "1. **Import Libraries and Helper Functions**\n",
    "2. **Load and Explore the NASA Turbofan Engine Dataset**\n",
    "3. **Data Visualization**\n",
    "4. **Data Preprocessing for LSTM**\n",
    "5. **Create and Build LSTM Model**\n",
    "6. **Train the Model**\n",
    "7. **Model Export and Conversion**\n",
    "8. **Model Evaluation**\n",
    "9. **Compiling the Model for AURIX&trade;**\n",
    "10. **Evaluate Execution Timing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928cef68",
   "metadata": {},
   "source": [
    "## Import Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path so we can import from central_scripts\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from CentralScripts.helper_functions import *\n",
    "from modelling_helper import *\n",
    "\n",
    "origin = \"torch\"\n",
    "\n",
    "train_data_path = \"data/train_FD001.txt\"\n",
    "test_data_path = \"data/test_FD001.txt\"\n",
    "test_RUL_path = \"data/RUL_FD001.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2e3d5",
   "metadata": {},
   "source": [
    "## Load and Investigate the NASA Turbofan Engine Dataset\n",
    "\n",
    "This project uses the **NASA Turbofan Engine Degradation Simulation Dataset** (also known as the NASA C-MAPSS dataset). The dataset contains:\n",
    "\n",
    "- **Training data**: Run-to-failure time series data from multiple aircraft engines\n",
    "- **Test data**: Partial time series data for engines with unknown remaining useful life\n",
    "- **Ground truth**: Actual RUL values for the test engines\n",
    "\n",
    "The data loading process includes:\n",
    "- **Automatic download**: If data files are not found locally, they are automatically downloaded from the official NASA repository\n",
    "- **Data normalization**: For better model training the train data is normalized\n",
    "- **Feature filtering**: Remove features that do not carry meaningful information by checking the standard deviation of the data. Features with a standard deviation lower than 0.02 are excluded\n",
    "\n",
    "**Data Source**: The dataset is automatically downloaded from the [NASA Prognostics Data Repository](https://www.nasa.gov/content/prognostics-center-of-excellence-data-set-repository) when needed.\n",
    "\n",
    " A. Saxena and K. Goebel (2008). “Turbofan Engine Degradation Simulation Data Set”, NASA Prognostics Data Repository, NASA Ames Research Center, Moffett Field, CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e19572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the training data\n",
    "# Note: If data files are not found locally, they will be automatically downloaded from NASA\n",
    "train_data = load_dataframe(train_data_path)\n",
    "train_data, scaler = normalize_data(train_data)\n",
    "\n",
    "removed_cols = clean_data(train_data)\n",
    "train_data.drop(removed_cols, axis=1, inplace=True)\n",
    "train_data = add_rul(train_data)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Removed columns: {removed_cols}\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f9c5b",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "- Time series patterns\n",
    "- RUL distribution\n",
    "- Sensor correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7a3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_cols = [\n",
    "    col for col in train_data.columns if \"sensor\" in col or \"op_setting\" in col\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(sensor_cols), 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "for i, sensor in enumerate(sensor_cols):\n",
    "    for unit in train_data[\"unit_number\"].unique():\n",
    "        unit_data = train_data[train_data[\"unit_number\"] == unit]\n",
    "        axes[i].plot(unit_data[\"time_in_cycles\"], unit_data[sensor])\n",
    "    axes[i].set_ylabel(sensor)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time in Cycles\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d5dcf7",
   "metadata": {},
   "source": [
    "## Data Preprocessing for LSTM\n",
    "\n",
    "- Turn into sequences of a defined length. The training data will have the shape `n_samples * sequence_length * n_features`\n",
    "- Split into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dd9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sequence_length = 50\n",
    "# Prepare sequences\n",
    "X, y = prepare_train_sequences(train_data, sequence_length)\n",
    "\n",
    "# Split the data into 90% training and 10% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"Sequence length: {X_train.shape[1]}\")\n",
    "print(f\"Number of features: {X_train.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c89c7c",
   "metadata": {},
   "source": [
    "## Create and Build LSTM Model\n",
    "\n",
    "- set `num_layers` and `hidden_size` to tweak the architecture and test its effects on predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ef12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# LSTM architecture parameters\n",
    "input_size = X_train.shape[2]  # Number of features\n",
    "hidden_size = 16  # Hidden units in LSTM\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "output_size = 1  # Single RUL value\n",
    "dropout = 0.05  # Dropout rate\n",
    "\n",
    "model = LSTMmodel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    output_size=output_size,\n",
    "    dropout_rate=dropout,\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model type: {model.model_type}\")\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f2194",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "- The training process uses early stopping to avoid overfitting.\n",
    "- To ensure proper convergence learning rate reduction is used when training stalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use original input data for LSTM (no flattening)\n",
    "X_train_final = X_train\n",
    "X_val_final = X_val\n",
    "\n",
    "print(\"Using original input for LSTM:\")\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"X_val_final shape: {X_val_final.shape}\")\n",
    "\n",
    "train_loss, val_loss, lr = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val_final,\n",
    "    y_val=y_val,\n",
    "    initial_lr=0.01,\n",
    "    num_epochs=10,  # Reduced for faster training in demo\n",
    ")\n",
    "\n",
    "plot_training_progress(train_loss, val_loss, lr)\n",
    "\n",
    "# Load the best model - check if file exists first\n",
    "checkpoint_path = \"model_checkpoints/best_model.pth\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    print(\"Best model loaded.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Using current model state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c9a06c",
   "metadata": {},
   "source": [
    "# Export Model to ONNX\n",
    "\n",
    "- Generate a model name based on its architecture and loss\n",
    "- Save the model together with a test in- & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model name\n",
    "model_name = f\"{model.model_type}_layers-{num_layers}_loss-{min(val_loss):.1f}\"\n",
    "model_name = model_name.replace(\".\", \"-\")\n",
    "\n",
    "# Prepare input and output for model export\n",
    "input_target = X_train[0].astype(np.float32)  # Single sequence (no flattening for LSTM)\n",
    "output_target = get_predictions(origin, model, input_target)\n",
    "\n",
    "save_all(model_name, input_target, output_target, model, origin)\n",
    "\n",
    "print(f\"Model saved as: {model_name}\")\n",
    "print(f\"Input shape for export: {input_target.shape}\")\n",
    "print(f\"Output shape: {output_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a1b29",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "### Load Test Data\n",
    "\n",
    "- Data that was not used during model training.\n",
    "- Normalize it with the scaler from the train data\n",
    "- Generate sequences of the same length as for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceff644",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_dataframe(test_data_path)\n",
    "test_RUL = pd.read_csv(test_RUL_path, names=[\"RUL\"], header=None)\n",
    "test_data, scaler = normalize_data(test_data, scaler)\n",
    "\n",
    "test_data.drop(removed_cols, axis=1, inplace=True)\n",
    "\n",
    "X_test, y_test = prepare_test_sequences(test_data, test_RUL, sequence_length)\n",
    "\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2623c1",
   "metadata": {},
   "source": [
    "### Make Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632e658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder, onnx_model_file = get_output_paths(model_name)\n",
    "y_pred_test = pd.DataFrame(\n",
    "    {\n",
    "        \"y_pred\": predict_with_onnx(onnx_model_file, X_test).reshape(\n",
    "            -1,\n",
    "        ),\n",
    "        \"y\": y_test,\n",
    "        \"Train_test\": \"Test\",\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred_train = pd.DataFrame(\n",
    "    {\n",
    "        \"y_pred\": predict_with_onnx(onnx_model_file, X_train).reshape(\n",
    "            -1,\n",
    "        ),\n",
    "        \"y\": y_train,\n",
    "        \"Train_test\": \"Train\",\n",
    "    }\n",
    ")\n",
    "\n",
    "y_pred = pd.concat([y_pred_train, y_pred_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1891ea",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "- Mean squared error (MSE) as loss\n",
    "- R2 as statistical metric for the predictive quality of the model, R2 = 1 indicating a perfect model\n",
    "- Compare the performance on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb8960",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_df = calculate_kpis(y_pred)\n",
    "print(\"Model Performance Metrics:\")\n",
    "print(kpi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2658366",
   "metadata": {},
   "source": [
    "### Plot true vs. predicted RUL\n",
    "\n",
    "- Assess model performance by directly comparing true and predicted values\n",
    "- Better correlation indicates a better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_vs_predicted(y_pred, kpi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93162308",
   "metadata": {},
   "source": [
    "# Convert Model for AURIX&trade; TC3x and TC4x deployment\n",
    "\n",
    "- Make sure the docker container with conversion tools is running\n",
    "- Submit the model together with test data to the container and download the generated code, binary and log files.\n",
    "- Results are saved in the `out/<model_name>/test_<model_name>/<target>/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_docker_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30bccc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CentralScripts.python_flask_client import CallTools\n",
    "\n",
    "model_folder, onnx_model_file = get_output_paths(model_name)\n",
    "\n",
    "for target in [\"TC3\", \"TC4\"]:\n",
    "    tool = CallTools(\n",
    "        folder=model_folder, url=\"http://localhost:8080/convert\", target=target\n",
    "    )\n",
    "    tool.convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8a761b",
   "metadata": {},
   "source": [
    "## Simulated instruction counts\n",
    "\n",
    "- For each node in the neural network the number of instruction counts is extracted from the log file and plotted.\n",
    "- You can inspect which node is a computational bottleneck and adjust your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf8735",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_instruction_counts(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97378a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
