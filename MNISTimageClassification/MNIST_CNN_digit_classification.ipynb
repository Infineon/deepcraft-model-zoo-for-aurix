{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493187f5",
   "metadata": {},
   "source": [
    "### Copyright (C) Infineon Technologies AG 2025\n",
    " \n",
    "Copyright (c) 2025, Infineon Technologies AG, or an affiliate of Infineon Technologies AG. All rights reserved.\n",
    "This software, associated documentation and materials (\"Software\") is owned by Infineon Technologies AG or one of its affiliates (\"Infineon\") and is protected by and subject to worldwide patent protection, worldwide copyright laws, and international treaty provisions. Therefore, you may use this Software only as provided in the license agreement accompanying the software package from which you obtained this Software. If no license agreement applies, then any use, reproduction, modification, translation, or compilation of this Software is prohibited without the express written permission of Infineon.\n",
    "\n",
    "Disclaimer: UNLESS OTHERWISE EXPRESSLY AGREED WITH INFINEON, THIS SOFTWARE IS PROVIDED AS-IS, WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ALL WARRANTIES OF NON-INFRINGEMENT OF THIRD-PARTY RIGHTS AND IMPLIED WARRANTIES SUCH AS WARRANTIES OF FITNESS FOR A SPECIFIC USE/PURPOSE OR MERCHANTABILITY. Infineon reserves the right to make changes to the Software without notice. You are responsible for properly designing, programming, and testing the functionality and safety of your intended application of the Software, as well as complying with any legal requirements related to its use. Infineon does not guarantee that the Software will be free from intrusion, data theft or loss, or other breaches (\"Security Breaches\"), and Infineon shall have no liability arising out of any Security Breaches. Unless otherwise explicitly approved by Infineon, the Software may not be used in any application where a failure of the Product or any consequences of the use thereof can reasonably be expected to result in personal injury."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8d090",
   "metadata": {},
   "source": [
    "# MNIST CNN Digit Classification\n",
    "\n",
    "This notebook demonstrates how to build and train a Convolutional Neural Network (CNN) for handwritten digit classification using the MNIST dataset and the helper functions from `modelling_helper.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ce28d-72a7-43f8-a06c-1d11c16b7f69",
   "metadata": {},
   "source": [
    "### Notebook Structure\n",
    "\n",
    "1. **Import Libraries and Helper Functions**\n",
    "2. **Load and Explore the MNIST Dataset**\n",
    "3. **Visualize Sample Images**\n",
    "4. **Data Preprocessing**\n",
    "5. **Create and Build CNN Model**\n",
    "6. **Train the Model**\n",
    "7. **Evaluate Model Performance**\n",
    "8. **Model Summary**\n",
    "9. **Compiling the Model for AURIX&trade;**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16b879",
   "metadata": {},
   "source": [
    "## Import Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "from CentralScripts.helper_functions import *\n",
    "from modelling_helper import *\n",
    "\n",
    "origin = \"tf\"  # \"tf\" stands for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dc90bf",
   "metadata": {},
   "source": [
    "## Load and Explore the MNIST Dataset\n",
    "\n",
    "This project uses the **MNIST Handwritten Digit Dataset**, a classic benchmark in machine learning and computer vision. The dataset contains:\n",
    "\n",
    "- **Training data**: 60,000 handwritten digit images (28x28 pixels)\n",
    "- **Test data**: 10,000 handwritten digit images for evaluation\n",
    "- **10 classes**: Digits 0 through 9\n",
    "- **Grayscale images**: Single channel, normalized pixel values\n",
    "\n",
    "### Dataset Details\n",
    "\n",
    "- **Image size**: 28×28 pixels, grayscale\n",
    "- **Total samples**: 70,000 images\n",
    "- **Classes**: 10 digit classes (0-9)\n",
    "- **Format**: Preprocessed and normalized pixel values\n",
    "- **Data split**: 54,000 training, 6,000 validation, 10,000 test samples\n",
    "\n",
    "The MNIST dataset provides a controlled environment for demonstrating fundamental computer vision techniques that scale to more complex automotive applications. The small image size and computational requirements make it ideal for embedded system deployment validation.\n",
    "\n",
    "**Data Source**: The dataset is automatically downloaded through TensorFlow/Keras and is part of the standard machine learning benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe48ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "data = load_mnist_data(val_size=0.1)\n",
    "\n",
    "x_train = data[\"x_train\"]\n",
    "y_train = data[\"y_train\"]\n",
    "x_val = data[\"x_val\"]\n",
    "y_val = data[\"y_val\"]\n",
    "x_test = data[\"x_test\"]\n",
    "y_test = data[\"y_test\"]\n",
    "\n",
    "print(f\"Training set: {x_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {x_val.shape[0]} samples\")\n",
    "print(f\"Test set: {x_test.shape[0]} samples\")\n",
    "print(f\"Image shape: {x_train.shape[1:]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148c87da",
   "metadata": {},
   "source": [
    "## Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample images\n",
    "plot_sample_images(x_train, y_train, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77eb328",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12da7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to 0-1 range\n",
    "x_train_norm, x_val_norm, x_test_norm = normalize_data(x_train, x_val, x_test)\n",
    "\n",
    "print(f\"Original pixel range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Normalized pixel range: [{x_train_norm.min()}, {x_train_norm.max()}]\")\n",
    "\n",
    "# Prepare data for CNN by adding channel dimension\n",
    "x_train_cnn, x_val_cnn, x_test_cnn = prepare_cnn_data(\n",
    "    x_train_norm, x_val_norm, x_test_norm\n",
    ")\n",
    "\n",
    "print(f\"CNN input shape: {x_train_cnn.shape[1:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680e205",
   "metadata": {},
   "source": [
    "## Create and Build CNN Model\n",
    "\n",
    "### Convolutional Neural Network (CNN)\n",
    "- **Architecture**: Convolutional layers with pooling and dense classification head\n",
    "- **Input**: 28×28×1 images preserving spatial structure\n",
    "- **Features**: Conv2D layers, MaxPooling, dropout, and dense layers\n",
    "- **Output**: 10-class softmax classification\n",
    "- **Use case**: Higher accuracy applications, spatial feature extraction\n",
    "\n",
    "- Tweak the `conv_layers` list to test the effect of the model architecture on predictions. The first number of each tuple is the number of filters and the second the kernel size\n",
    "- `dense_units` specifies the units of the dense layers following the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acefeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output shapes for CNN\n",
    "input_shape, output_shape = get_input_output_shapes(x_train, y_train, model_type=\"cnn\")\n",
    "\n",
    "# Define CNN architecture\n",
    "conv_layers = [(8, 3), (16, 3)]  # (filters, kernel_size)\n",
    "dense_units = [16]  # Dense layer units\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = create_cnn_model(\n",
    "    input_shape=input_shape,\n",
    "    output_shape=output_shape,\n",
    "    conv_layers=conv_layers,\n",
    "    dense_units=dense_units,\n",
    "    p_drop=dropout_rate,\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bbffdf",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "- The training process uses early stopping to avoid overfitting.\n",
    "- To ensure proper convergence learning rate reduction is used when training stalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa817ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    x_train=x_train_cnn,\n",
    "    y_train=y_train,\n",
    "    x_val=x_val_cnn,\n",
    "    y_val=y_val,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    ")\n",
    "\n",
    "plot_training_history(history, model_name=\"CNN Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa42758",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = evaluate_model(model, x_test_cnn, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    model, x_test_cnn, y_test, class_names=[str(i) for i in range(10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c43c6b",
   "metadata": {},
   "source": [
    "# Export Model to ONNX\n",
    "\n",
    "- Generate a model name based on its architecture and loss\n",
    "- Save the model together with a test in- & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a03b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = generate_model_name(model, test_accuracy, test_loss)\n",
    "\n",
    "# Prepare input and output for model export (following RUL pattern)\n",
    "input_target = x_train_cnn[0]  # Shape: (28, 28, 1) - unbatched\n",
    "output_target = get_predictions(origin, model, input_target)\n",
    "\n",
    "save_all(model_name, input_target, output_target, model, origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ec3f5",
   "metadata": {},
   "source": [
    "# Convert Model for AURIX&trade; TC3x and TC4x deployment\n",
    "\n",
    "- Make sure the docker container with conversion tools is running\n",
    "- Submit the model together with test data to the container and download the generated code, binary and log files.\n",
    "- Results are saved in the `out/<model_name>/test_<model_name>/<target>/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70afdd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_docker_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CentralScripts.python_flask_client import CallTools\n",
    "\n",
    "model_folder, onnx_model_file = get_output_paths(model_name)\n",
    "\n",
    "for target in [\"TC3\", \"TC4\"]:\n",
    "    tool = CallTools(\n",
    "        folder=model_folder, url=\"http://localhost:8080/convert\", target=target\n",
    "    )\n",
    "    tool.convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ed08a",
   "metadata": {},
   "source": [
    "## Simulated instruction counts\n",
    "\n",
    "- For each node in the neural network the number of instruction counts is extracted from the log file and plotted.\n",
    "- You can inspect which node is a computational bottleneck and adjust your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_instruction_counts(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
