{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright (C) Infineon Technologies AG 2025\n",
    " \n",
    "Copyright (c) 2025, Infineon Technologies AG, or an affiliate of Infineon Technologies AG. All rights reserved.\n",
    "This software, associated documentation and materials (\"Software\") is owned by Infineon Technologies AG or one of its affiliates (\"Infineon\") and is protected by and subject to worldwide patent protection, worldwide copyright laws, and international treaty provisions. Therefore, you may use this Software only as provided in the license agreement accompanying the software package from which you obtained this Software. If no license agreement applies, then any use, reproduction, modification, translation, or compilation of this Software is prohibited without the express written permission of Infineon.\n",
    "\n",
    "Disclaimer: UNLESS OTHERWISE EXPRESSLY AGREED WITH INFINEON, THIS SOFTWARE IS PROVIDED AS-IS, WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ALL WARRANTIES OF NON-INFRINGEMENT OF THIRD-PARTY RIGHTS AND IMPLIED WARRANTIES SUCH AS WARRANTIES OF FITNESS FOR A SPECIFIC USE/PURPOSE OR MERCHANTABILITY. Infineon reserves the right to make changes to the Software without notice. You are responsible for properly designing, programming, and testing the functionality and safety of your intended application of the Software, as well as complying with any legal requirements related to its use. Infineon does not guarantee that the Software will be free from intrusion, data theft or loss, or other breaches (\"Security Breaches\"), and Infineon shall have no liability arising out of any Security Breaches. Unless otherwise explicitly approved by Infineon, the Software may not be used in any application where a failure of the Product or any consequences of the use thereof can reasonably be expected to result in personal injury."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Template\n",
    "\n",
    "This is a template for adding a new model that is already trained and ready for inference. Please run this first (to make sure everything works on your machine) and then navigate through the TO DOs to add your model and data step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:41:24.035727Z",
     "start_time": "2025-06-12T09:41:10.351911Z"
    }
   },
   "outputs": [],
   "source": [
    "# TO DO: remove libs you don't need and add libs you need\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the Python path so we can import from CentralScripts\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "import CentralScripts.helper_functions as cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a Model\n",
    "\n",
    "The purpose of the model is to demonstrate compilation for AURIX&trade;. You are invited to replace this with your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:41:26.691139Z",
     "start_time": "2025-06-12T09:41:26.683661Z"
    }
   },
   "outputs": [],
   "source": [
    "from modelling_helper import get_model\n",
    "\n",
    "# TO DO: add your model in modelling helper\n",
    "origin = \"tf\"  # choose your origin, e.g., \"tf\" for TensorFlow, \"torch\" for PyTorch\n",
    "\n",
    "# TO DO: choose your model_name\n",
    "model_name = \"new_model\"\n",
    "\n",
    "# TO DO: add your model\n",
    "model = get_model(origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding Input Data\n",
    "\n",
    "You could use random input data or import your input data here and convert it into a numpy array.\n",
    "We will run inference on it to generate the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:41:29.353640Z",
     "start_time": "2025-06-12T09:41:29.327523Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TO DO: add an exemplary input as numpy array\n",
    "input_target = np.random.rand(10).astype(np.float32)  # Example input for the model\n",
    "output_target = cs.get_predictions(origin, model, input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export and Save Model and Data\n",
    "\n",
    "We now export model, input, and output data: \n",
    "- the model will be converted into ONNX\n",
    "- input, and output will be stored as .pb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:41:31.769556Z",
     "start_time": "2025-06-12T09:41:31.547781Z"
    }
   },
   "outputs": [],
   "source": [
    "# export model and data as ONNX and .pb\n",
    "cs.save_all(model_name, input_target, output_target, model, origin)\n",
    "cs.test_onnx_pb(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the Model for AURIX&trade;\n",
    "\n",
    "All the tools we need are included in a docker container. We fetch / start the Docker container next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:41:49.948279Z",
     "start_time": "2025-06-12T09:41:49.738226Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if the docker container is running. If not, fetch and/ or start it.\n",
    "cs.ensure_docker_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate a C file from the ONNX and convert this into an elf file next. The elf file is executed using the open-source hardware emulator QEMU to estimate the execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T09:26:44.533632Z",
     "start_time": "2025-06-12T09:26:08.395833Z"
    }
   },
   "outputs": [],
   "source": [
    "from CentralScripts.python_flask_client import CallTools\n",
    "\n",
    "model_folder, onnx_model_file = cs.get_output_paths(model_name)\n",
    "\n",
    "for target in [\"TC3\", \"TC4\"]:\n",
    "    tool = CallTools(\n",
    "        folder=model_folder, url=\"http://localhost:8080/convert\", target=target\n",
    "    )\n",
    "    tool.convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Execution Timing\n",
    "\n",
    "- For each node in the neural network the number of clock cycles is extracted from the log file and plotted.\n",
    "- You can inspect which node is a computational bottleneck and adjust your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.plot_execution_timing(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Congrats on Compiling your Model. \n",
    "If you got here without any error, it looks like you made it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
