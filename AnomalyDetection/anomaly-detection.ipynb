{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3fd47ff",
   "metadata": {},
   "source": [
    "### Copyright (C) Infineon Technologies AG 2025\n",
    " \n",
    "Copyright (c) 2025, Infineon Technologies AG, or an affiliate of Infineon Technologies AG. All rights reserved.\n",
    "This software, associated documentation and materials (\"Software\") is owned by Infineon Technologies AG or one of its affiliates (\"Infineon\") and is protected by and subject to worldwide patent protection, worldwide copyright laws, and international treaty provisions. Therefore, you may use this Software only as provided in the license agreement accompanying the software package from which you obtained this Software. If no license agreement applies, then any use, reproduction, modification, translation, or compilation of this Software is prohibited without the express written permission of Infineon.\n",
    "\n",
    "Disclaimer: UNLESS OTHERWISE EXPRESSLY AGREED WITH INFINEON, THIS SOFTWARE IS PROVIDED AS-IS, WITH NO WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, ALL WARRANTIES OF NON-INFRINGEMENT OF THIRD-PARTY RIGHTS AND IMPLIED WARRANTIES SUCH AS WARRANTIES OF FITNESS FOR A SPECIFIC USE/PURPOSE OR MERCHANTABILITY. Infineon reserves the right to make changes to the Software without notice. You are responsible for properly designing, programming, and testing the functionality and safety of your intended application of the Software, as well as complying with any legal requirements related to its use. Infineon does not guarantee that the Software will be free from intrusion, data theft or loss, or other breaches (\"Security Breaches\"), and Infineon shall have no liability arising out of any Security Breaches. Unless otherwise explicitly approved by Infineon, the Software may not be used in any application where a failure of the Product or any consequences of the use thereof can reasonably be expected to result in personal injury."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5944bc",
   "metadata": {},
   "source": [
    "# Anomaly Detection with MLP Autoencoder\n",
    "\n",
    "This notebook demonstrates how to build and train a Multi-Layer Perceptron (MLP) based autoencoder for anomaly detection using the Controlled Anomalies Time Series (CATS) dataset and the helper functions from `modelling_helper.py`.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Anomaly detection is a critical capability for monitoring complex systems and identifying unusual patterns that may indicate equipment failures, security breaches, or operational issues. In industrial applications, effective anomaly detection provides:\n",
    "\n",
    "- **Predictive maintenance** - Early detection of equipment degradation before catastrophic failure\n",
    "- **Quality assurance** - Identification of process deviations that affect product quality\n",
    "- **Security monitoring** - Detection of unusual patterns that may indicate cyberattacks or system intrusions\n",
    "- **Operational optimization** - Identification of inefficient or suboptimal system behaviors\n",
    "- **Cost reduction** - Prevention of costly downtime through early anomaly identification\n",
    "\n",
    "## Autoencoder Approach\n",
    "\n",
    "This notebook uses an **autoencoder neural network** approach for anomaly detection:\n",
    "\n",
    "- **Training Phase** - The autoencoder learns to reconstruct normal system behavior from multivariate time series data\n",
    "- **Detection Phase** - Anomalies are identified by measuring reconstruction error - high reconstruction error indicates anomalous behavior\n",
    "- **Unsupervised Learning** - The model learns normal patterns without requiring labeled anomalous examples during training\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "This notebook walks through the complete anomaly detection workflow:\n",
    "\n",
    "1. **Data Loading & Exploration** - Load the CATS dataset, understand its structure, and visualize time series patterns\n",
    "2. **Data Preprocessing** - Normalize features, create time series sequences, and prepare training/validation splits\n",
    "3. **Model Definition** - Build an MLP-based autoencoder architecture optimized for time series reconstruction\n",
    "4. **Model Training** - Train the autoencoder on normal data with early stopping and learning rate scheduling\n",
    "5. **Model Evaluation** - Assess reconstruction performance and validate anomaly detection capability\n",
    "6. **Model Export** - Convert the trained model to ONNX format for deployment\n",
    "7. **Hardware Deployment** - Compile the model for AURIX&trade; microcontroller deployment\n",
    "8. **Performance Analysis** - Evaluate execution timing and resource usage on target hardware\n",
    "\n",
    "The methodologies demonstrated are applicable to various industrial monitoring scenarios including manufacturing processes, power systems, transportation networks, and IoT sensor networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115bc15d",
   "metadata": {},
   "source": [
    "## Import Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for importing central scripts\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import project modules\n",
    "import CentralScripts.helper_functions as cs\n",
    "import modelling_helper as mh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a95df5be1605f7",
   "metadata": {},
   "source": [
    "# Load and Visualize Data\n",
    "\n",
    "## Dataset: Controlled Anomalies Time Series (CATS)\n",
    "\n",
    "This notebook uses the **Controlled Anomalies Time Series (CATS) Dataset** for anomaly detection model training. The CATS dataset is a synthetic multivariate time series dataset specifically designed for benchmarking anomaly detection algorithms.\n",
    "\n",
    "**Dataset Source:** [https://zenodo.org/records/8338435](https://zenodo.org/records/8338435)\n",
    "\n",
    "### Key Dataset Characteristics:\n",
    "\n",
    "- **Multivariate:** 17 variables including:\n",
    "  - 4 Control commands/actuations\n",
    "  - 3 Environmental stimuli/external forces  \n",
    "  - 10 Telemetry sensor readings (position, temperature, pressure, etc.)\n",
    "\n",
    "- **Scale:** 5 million timestamps at 1Hz sampling frequency\n",
    "  - First 1 million: Nominal (normal) behavior for learning baseline\n",
    "  - Last 4 million: Mixed nominal and anomalous segments for evaluation\n",
    "\n",
    "- **Anomalies:** 200 precisely labeled anomalous segments with different types and known ground truth\n",
    "- **Root Cause Analysis:** Metadata includes root cause channels and affected channels for each anomaly\n",
    "- **Clean Signals:** Noise-free data allows controlled robustness testing\n",
    "\n",
    "The dataset simulates a complex dynamical system and is particularly suitable for:\n",
    "- Semi-supervised anomaly detection (novelty detection)\n",
    "- Unsupervised anomaly detection (outlier detection)  \n",
    "- Root cause analysis and explainability evaluation\n",
    "- Algorithm robustness testing\n",
    "\n",
    "### Citation\n",
    "\n",
    "```\n",
    "Patrick Fleith. (2023). Controlled Anomalies Time Series (CATS) Dataset (Version 2) \n",
    "[Data set]. Solenix Engineering GmbH. https://doi.org/10.5281/zenodo.8338435\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d134d63d36ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train, test = mh.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130b91a",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a419d7e64fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.plot_time_steps(\n",
    "    df=train,\n",
    "    n_steps=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262ea6b",
   "metadata": {},
   "source": [
    "# Prepare input sequences for model training\n",
    "\n",
    "- `sequence_length` defines the number of time steps takes as input for the model. Adjust this to see its effect on detection quality. Larger `seqence_length` requires a larger input layer of the auto encoder and therefore results in overall larger models requiring more memory.\n",
    "- From the train data random sequences of length `sequence_length` are selected. A higher number ensures better coverage of the normal behaviour of the train data but increases training time.\n",
    "- Input data is proved as a tensor of sequences with shape `n_samples * sequence_length * n_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd865e97eedaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_scaled, scaler = mh.normalize_data(df=train)\n",
    "test_scaled, scaler = mh.normalize_data(df=test, scaler=scaler)\n",
    "\n",
    "# seelct the length of sequences\n",
    "sequence_length = 30\n",
    "\n",
    "sequences = mh.sample_sequences(\n",
    "    df=train_scaled,\n",
    "    sequence_length=sequence_length,\n",
    "    n_samples=10000,  # Use a smaller number for quicker testing\n",
    ")\n",
    "\n",
    "# Split sequences: 95% train, 5% validation\n",
    "sequences_train, sequences_val = train_test_split(\n",
    "    sequences, test_size=0.05, random_state=42\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train sequences: {len(sequences_train)}, Validation sequences: {len(sequences_val)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a174f1",
   "metadata": {},
   "source": [
    "# Build an MLP based autoencoder\n",
    "\n",
    "- Set the number of layers: it will be the same number on both encoder and decoder sides. A single layer proved to get good results\n",
    "- The bottleneck defines the compression of the data. Play with this number to see what works best.\n",
    "- As loss the Mean Squared Error (MSE) is used. Model performance later is assessed using this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35dd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mh.mlp_autoencoder(\n",
    "    input_dim=sequences_train.shape[1] * sequences_train.shape[2],\n",
    "    bottleneck=64,\n",
    "    layers=1,\n",
    "    p_drop=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d10be",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "- The training process uses early stopping to avoid overfitting.\n",
    "- To ensure proper convergence learning rate reduction is used when training stalls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85315a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequences_train.reshape(sequences_train.shape[0], -1)\n",
    "X_val = sequences_val.reshape(sequences_val.shape[0], -1)\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=X_train,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=[X_val, X_val],\n",
    "    callbacks=mh.get_callbacks(),\n",
    ")\n",
    "cs.plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd77e2cc",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "- Make model predictions on the train & test data sets and calculate the reconstruction error as Mean Squared Error\n",
    "- Compare the MSE distributions for train and test data -> both distributions should strongly overlap!\n",
    "- Define an anomaly threshold `thresh` for the MSE above which to consider a sequence as anomalous. Typically, this is set to the 95th or above percentile of the MSE of the train or test set. Adjusting this threshold shifts the balance between false positive and false negative detections.\n",
    "- Check model functionality on anomaly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09fdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y_train = model.predict(X_train)\n",
    "err_train = np.sum((y_train - X_train) ** 2, axis=1)\n",
    "\n",
    "res_train = pd.DataFrame({\"MSE\": err_train, \"Train_test\": \"Train\", \"Label\": \"clean\"})\n",
    "\n",
    "test_scaled, scaler = mh.normalize_data(df=test, scaler=scaler)\n",
    "\n",
    "sequences_test = mh.sample_sequences(\n",
    "    df=test_scaled,\n",
    "    sequence_length=sequence_length,\n",
    "    n_samples=5000,\n",
    ")\n",
    "\n",
    "X_test = sequences_test.reshape(sequences_test.shape[0], -1)\n",
    "\n",
    "y_test = model.predict(X_test)\n",
    "err_test = np.sum((y_test - X_test) ** 2, axis=1)\n",
    "\n",
    "res_test = pd.DataFrame({\"MSE\": err_test, \"Train_test\": \"Test\", \"Label\": \"clean\"})\n",
    "\n",
    "res = pd.concat([res_train, res_test], ignore_index=True)\n",
    "\n",
    "\n",
    "quantile = 0.95  # 95th percentile, adjust as needed\n",
    "threshold = res_train[\"MSE\"].quantile(quantile)\n",
    "\n",
    "print(f\"Set anomaly threshold at: {threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e3bc2",
   "metadata": {},
   "source": [
    "## Plot reconstruction error distributions\n",
    "\n",
    "- Mean squared error (MSE) as metric as used for model training loss.\n",
    "- MSE for anomaly free data from train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef14e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.plot_reconstruction_error(res, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543615f6",
   "metadata": {},
   "source": [
    "## Check detection quality\n",
    "\n",
    "- Plot an anomalous time series.\n",
    "- The meta data data frame contains the information for 200 labelled anomalies from the test data set. Select one and see the detection quality.\n",
    "- Calculate the MSE at each time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3ac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(\"data/metadata.csv\")\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e52756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# selecting an anomaly from the meta data (0 - 199)\n",
    "ii = 11\n",
    "\n",
    "start = meta_data[\"start_time\"][ii]\n",
    "start = pd.to_datetime(start)\n",
    "end = meta_data[\"end_time\"][ii]\n",
    "end = end = pd.to_datetime(end)\n",
    "\n",
    "root_cause = meta_data[\"root_cause\"][ii]\n",
    "affected = meta_data[\"affected\"][ii]\n",
    "affected = re.findall(r\"'([^']*)'\", affected)[0]\n",
    "\n",
    "print(f\"Anomaly from {start} to {end}, root cause: {root_cause}, affected: {affected}\")\n",
    "\n",
    "# Load the anomalous data directly from data.csv\n",
    "anomaly_rows = mh.load_anomaly_data_from_csv(\"data/data.csv\", start, end)\n",
    "\n",
    "# Set timestamp as index for easier manipulation\n",
    "anomaly_rows = anomaly_rows.set_index(\"timestamp\")\n",
    "\n",
    "# Normalize the anomaly data using the same scaler\n",
    "anomaly_rows_scaled = mh.normalize_data(df=anomaly_rows, scaler=scaler)[0]\n",
    "\n",
    "# Update anomaly_data dictionary with the new data\n",
    "anomaly_data = {\n",
    "    \"rows\": anomaly_rows,\n",
    "    \"rows_scaled\": anomaly_rows_scaled,\n",
    "    \"start\": start,\n",
    "    \"end\": end,\n",
    "    \"root_cause\": root_cause,\n",
    "    \"affected\": affected,\n",
    "}\n",
    "\n",
    "print(f\"Loaded anomaly data shape: {anomaly_rows.shape}\")\n",
    "print(f\"Time range: {anomaly_rows.index.min()} to {anomaly_rows.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ac10cb",
   "metadata": {},
   "source": [
    "## Create sequences of anomalous data for model predictions and calculate reconstruction error\n",
    "\n",
    "- Sequences as rolling window over the selected anomalous time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151471f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = mh.sequential_sequences(\n",
    "    df=anomaly_rows_scaled,\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "y_pred = model.predict(seqs.reshape(seqs.shape[0], -1))\n",
    "err = np.sum((y_pred - seqs.reshape(seqs.shape[0], -1)) ** 2, axis=1)\n",
    "\n",
    "err = pd.DataFrame({\"MSE\": err, \"timestamp\": anomaly_rows.index[sequence_length - 1 :]})\n",
    "\n",
    "anomaly_data[\"err\"] = err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbf6702",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "\n",
    "- At each time point plot the reconstruction error and compare it against the selected threshold to label anomalous time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21cafad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mh.plot_anomaly_detection(anomaly_data, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf52c9",
   "metadata": {},
   "source": [
    "# Export Model to ONNX\n",
    "\n",
    "- Generate a model name based on its architecture and loss\n",
    "- Save the model together with a test in- & output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d08a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = model.evaluate(X_test, X_test, verbose=0)\n",
    "\n",
    "model_name = mh.generate_model_name(model, test_loss)\n",
    "\n",
    "origin = \"tf\"\n",
    "\n",
    "input_target = sequences_train[0].reshape(-1).astype(np.float32)\n",
    "output_target = cs.get_predictions(origin, model, input_target)\n",
    "\n",
    "cs.save_all(model_name, input_target, output_target, model, origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba9189d",
   "metadata": {},
   "source": [
    "# Convert Model for AURIX&trade; TC3x/TC4x deployment\n",
    "\n",
    "- Make sure the docker container with conversion tools is running\n",
    "- Submit the model together with test data to the container and download the generated code, binary and log files.\n",
    "- Results are saved in the `out/<model_name>/test_<model_name>/<target>/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de5d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.ensure_docker_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b90375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CentralScripts.python_flask_client import CallTools\n",
    "\n",
    "model_folder, onnx_model_file = cs.get_output_paths(model_name)\n",
    "\n",
    "for target in [\"TC3\", \"TC4\"]:\n",
    "    tool = CallTools(\n",
    "        folder=model_folder, url=\"http://localhost:8080/convert\", target=target\n",
    "    )\n",
    "    tool.convert_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909cba78",
   "metadata": {},
   "source": [
    "## Simulated instruction counts\n",
    "\n",
    "- For each node in the neural network the number of instruction counts is extracted from the log file and plotted.\n",
    "- You can inspect which node is a computational bottleneck and adjust your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2180bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.plot_instruction_counts(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05380959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
